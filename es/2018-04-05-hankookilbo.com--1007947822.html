<div id="readability-page-1" class="page"><h3 class="title" data-translation="<span>El debate del &quot;robot asesino&quot; que se ha convertido en realidad</span>">현실이 돼 버린 ‘킬러 로봇’ 논쟁</h3><article>
                                        
                                        
                                        <p data-translation="<span>Inscripción: 2018.04.05 19:13 Revisión: 2018.04.05 19:38</span>">등록 : 2018.04.05 19:13
                                            
                                                수정 : 2018.04.05 19:38
                                                
                                            </p>
                                    </article><article id="article-body">

                                        

                                        <article>
                                            <div>
                                                <center>

                                        <img src="http://image.hankookilbo.com/i.aspx?Guid=3c74dd92cbce4befaf94d6fea41a1f01&amp;Month=201804&amp;size=640" alt="">

                                         </center>
                                            </div>

                                            <p data-translation="<span>Propaganda de la película 'Terminator: El comienzo de la guerra futura'</span>">영화 ‘터미네이터: 미래 전쟁의 시작’ 의 선전물</p>

                                        </article>
                                        
                                              
                                        
                                        <p data-translation="<span>&quot;Una máquina puede matar a un hombre por su propio juicio&quot;.</span>">“기계가 스스로의 판단 만으로 사람을 죽일 수 있나.”
</p><p data-translation="<span>La cuestión de la serie de Hollywood &quot;Terminator&quot;, que fue tratada solo como una película de ciencia ficción hace 34 años, surgió como un problema para la humanidad en 2018.</span>">34년전 공상과학영화로만 취급받았던 헐리우드 영화 ‘터미네이터’ 시리즈의 질문이 2018년 인류에게 실존 문제로 다가왔다.</p><p data-translation="<span>El deseo humano de un arma más fuerte comenzó a aumentar con la inteligencia artificial (<span id=&quot;_0&quot;>AI</span>), lo que llevó al desarrollo de armas para ser juzgadas e impulsadas por los científicos. Además, el centro de la controversia no es solo la empresa de vanguardia de Silicon Valley en los Estados Unidos, sino también el Instituto Coreano de Ciencia y Tecnología (KAIST), un símbolo de la ciencia y la tecnología coreanas.</span>">더 강한 무기에 대한 인간 욕망이 인공지능(<span id="_0" data-translation="AI">AI</span>)을 장착해 스스로 판단하고 구동하는 무기 개발로 쏠리면서, 과학자 집단에서 윤리 논쟁이 달아오르기 시작했다. 게다가 논쟁의 중심에는 미국 실리콘밸리의 최첨단 기업 구글은 물론이고 한국 과학기술의 상징인 한국과학기술원(카이스트)까지 휘말려 들고 있다.</p><p data-translation="<span>The New York Times (<span id=&quot;_1&quot;>NYT</span>) informó el martes (4 de abril ) una petición firmada por 3.100 empleados de Google, incluidos docenas de ingenieros de alto nivel , para enviar al<span id=&quot;_2&quot;>CEO</span>Sundar Pisai. El contenido fue simple &quot;Creemos que Google no debería participar en la guerra&quot;. La acción colectiva de los empleados de Google se debe al Pentágono de la compañía y al proyecto en curso 'Maven'. Maven es un programa piloto del Pentágono para tratar de mejorar el poder de ataque del caza no tripulado de la Fuerza Aérea de los EE. UU. (Drones) con la tecnología de<span id=&quot;_3&quot;>IA</span>basada en la nube de Google .</span>">뉴욕타임스(<span id="_1" data-translation="NYT">NYT</span>)는 4일(현지시간) 최고위급 엔지니어 수십명을 포함, 구글직원 3,100명이 서명해 순다르 피차이 최고경영자(<span id="_2" data-translation="CEO">CEO</span>)에게 보낼 청원서를 보도했다. 내용은 간단했다. “우리는 구글이 전쟁 사업에 참여해선 안 된다고 믿는다”였다. 구글 직원의 집단 행동은 이 회사가 미 국방부(펜타곤)와 진행 중인 ‘메이븐’ 프로젝트 때문이다. 메이븐은 구글의 클라우드 기반 인공지능(<span id="_3" data-translation="IA">AI</span>) 기술로 미 공군 무인전투기(드론)의 타격 능력 향상을 꾀하려는 펜타곤의 파일럿 프로그램이다.

</p><p data-translation="<span><span id=&quot;_4&quot;></span>Según el NYT , el mensaje de los ingenieros de Google es claro. No importa cómo apunte al enemigo, el desarrollo de la tecnología de<span id=&quot;_5&quot;>inteligencia artificial</span>, destinado a matar personas, está alineado con el objetivo de la organización de Google, &quot;<span id=&quot;_6&quot;>Do not</span> <span id=&quot;_7&quot;>be</span> <span id=&quot;_8&quot;>Evil</span>&quot;. &quot;Es inaceptable construir un sistema de vigilancia militar que pueda tener consecuencias catastróficas para cooperar con el gobierno&quot;, dijo.<span id=&quot;_9&quot;>El NYT</span>también analizó los &quot;choques culturales entre Silicon Valley y el gobierno que tienen tecnología avanzada en el uso creciente de<span id=&quot;_10&quot;>la</span>tecnología de inteligencia artificial para fines militares&quot;.</span>"><span id="_4" data-translation="">NYT</span>에 따르면 구글 엔지니어들의 메시지는 뚜렷하다. 아무리 적군을 대상으로 한다 해도 인명살상이 목적인 <span id="_5" data-translation="inteligencia artificial">AI</span> 기술 개발은 ‘사악해지지 말자(<span id="_6" data-translation="Do not">Don’t</span> <span id="_7" data-translation="be">be</span> <span id="_8" data-translation="Evil">Evil</span>)’인 구글 조직의 목표와 정면 배치된다는 얘기다. 이들은 “정부에 협력한다는 명분으로 군사적 감시, 나아가 치명적인 결과를 낳을 수도 있는 기술을 구축하는 것은 받아들일 수 없다”고 밝혔다. <span id="_9" data-translation="El NYT">NYT</span>도 “<span id="_10" data-translation="la">AI</span> 기술의 군사 목적 활용이 점점 늘어나는 상황에서 첨단 기술을 보유한 실리콘밸리와 정부 간에 벌어지는 문화적 충돌”이라고 분석했다.

</p><p data-translation="<span>Desafortunadamente, ese mismo día, Corea quedó atrapada en el debate del &quot;terminador&quot;. Cincuenta robots prominentes extranjeros enviaron una carta de advertencia a KAIST. El comienzo se debió al &quot;Centro de Investigación de Convergencia de Inteligencia Artificial de Defensa&quot; inaugurado conjuntamente por KAIST y Hanhwa Systems a finales de febrero. En una carta al British Financial Times (<span id=&quot;_11&quot;>FT</span>), los expertos en robótica han hablado directamente sobre &quot;las preocupaciones del robot asesino&quot;, diciendo: &quot;Hasta el compromiso de KAIST de desarrollar armas que determinen de forma autónoma sin control humano, Rechazaré por completo toda investigación colaborativa &quot;.</span>">공교롭게 같은 날 한국도 ‘터미네이터’ 논쟁에 휘말렸다. 외국의 저명한 로봇학자 50명이 카이스트에 경고 서한을 보낸 것. 발단은 카이스트와 한화시스템이 지난 2월 말 공동 개소한 ‘국방인공지능융합연구센터’ 때문이었다. 영국 파이낸셜타임스(<span id="_11" data-translation="FT">FT</span>)가 공개한 서한에서 로봇학자들은 ‘킬러로봇 우려’까지 직접 거론하며, “인간의 유의미한 통제 없이 자율적으로 결정하는 무기를 개발하지 않겠다는 확약을 카이스트 총장이 할 때까지 우리는 모든 공동연구를 전면 거부할 것”이라고 선언했다. 

</p><p data-translation="<span>KAIST no tiene intención de desarrollar un robot asesino. Un funcionario de la escuela dijo: &quot;El 19 de noviembre, le enviamos una carta al presidente, diciendo: 'No es una investigación sobre armas de destrucción masiva o el desarrollo de robots asesinos'. &quot;Volveremos a enfatizar que no realizaremos actividades de investigación que sean contrarias a la dignidad humana, como el desarrollo de armas autónomas que carecen de control&quot;.</span>">카이스트는 킬러 로봇 개발 의사가 전혀 없다는 입장이다. 학교 관계자는 “지난달 19일 ‘공격용 대량살상 무기나 킬러 로봇 개발 목적의 연구 수행이 아니다’라는 총장 명의의 공문을 보냈다”고 해명했다. 또 “통제력이 결여된 자율무기 개발 등 인간 존엄성에 어긋나는 연구활동을 수행하지 않을 것임을 다시 한번 강조한다”고 밝혔다.

</p><p data-translation="<span>Los expertos analizan la controversia en Google y KAIST como el comienzo de un debate &quot;terminador&quot; que durará por algún tiempo. En la Convención de la ONU sobre Armas Convencionales (<span id=&quot;_12&quot;>CCW</span>) en noviembre del año pasado , expertos de la ONU y ONG discutieron formas de prohibir las armas equipadas con<span id=&quot;_13&quot;>IA</span>, pero los ingenieros a cargo del desarrollo real plantearon cuestiones éticas oficiales Esto es porque esta es la primera vez.</span>">전문가들은 구글과 카이스트에서 벌어진 논쟁이 향후 상당기간 이어질 ‘터미네이터’ 논쟁의 시작으로 분석하고 있다. 지난해 11월 유엔 특정재래식무기금지협약(<span id="_12" data-translation="CCW">CCW</span>) 회의에서 <span id="_13" data-translation="IA">AI</span> 탑재 무기를 금지하는 방안이 유엔 전문가들과 시민단체를 중심으로 논의된 바 있지만, 실제 개발을 담당할 엔지니어들이 공식적으로 윤리 문제를 제기한 것은 이번이 처음이기 때문이다. 

</p><p data-translation="<span>Los activistas civiles reclaman fuertes restricciones legales a las armas de<span id=&quot;_14&quot;>IA</span>por razones humanitarias , pero los expertos muestran una actitud sombría. La codicia de la humanidad sobre la generosidad de<span id=&quot;_15&quot;>la</span>tecnología de inteligencia artificial , las buenas armas y la desconfianza hacia el enemigo, que es una competencia militar imparable. El destacado filósofo australiano Peter Singer dijo: &quot;Los contratistas militares conscientes del dinero y las rivalidades geopolíticas en todo el mundo pueden acelerar<span id=&quot;_16&quot;>la</span>competencia de las armas de IA &quot;. El ex subsecretario de Defensa de Estados Unidos , James Miller, también dijo: &quot;Estados Unidos no tiene las armas que<span id=&quot;_17&quot;>AI</span>juzgará automáticamente, pero si los enemigos como Rusia o China preparan poderosas armas equipadas con<span id=&quot;_18&quot;>inteligencia artificial</span>, la respuesta será diferente&quot;.</span>">시민운동가들은 인도주의적 차원에서 <span id="_14" data-translation="IA">AI</span> 무기에 대한 강력한 법적 규제를 주장하고 있으나, 전문가들은 우울한 전망을 내놓고 있다. <span id="_15" data-translation="la">AI</span> 기술의 범용성과 우수한 무기에 대한 인간의 탐욕, 그리고 적국에 대한 불신 때문에 멈출 수 없는 군비경쟁이 우려된다는 것이다. 호주의 저명한 철학자 피터 싱어는 “돈에 눈이 먼 군수업자, 각국의 지정학적 경쟁 등이 <span id="_16" data-translation="la">AI</span> 무기 경쟁을 가속화시킬 수 있다”고 말했다. 제임스 밀러 전 미국 국방부 차관보도 “미국은 <span id="_17" data-translation="AI">AI</span>가 자동 판단하는 무기를 갖지 않는 게 원칙이지만, 러시아 혹은 중국 같은 적국이 <span id="_18" data-translation="inteligencia artificial">AI</span>를 장착한 강력한 무기를 준비한다면 그 대응은 달라질 것”이라고 우려했다.

</p><p data-translation="<span>La pregunta ética: &quot;¿Es realmente posible que una máquina se juzgue a sí misma y mate a un ser humano?&quot; Es muy probable que<span id=&quot;_19&quot;>la</span>tecnología de inteligencia artificial se expanda hasta el infinito sin un control adecuado antes de que la humanidad encuentre la respuesta .</span>">‘기계가 스스로 판단해 인간을 살해하는 게 과연 온당한가’라는 윤리적 질문에 인류가 해답을 찾기도 전에 <span id="_19" data-translation="la">AI</span>기술이 적절한 통제 없이 무한대로 확장될 가능성이 높다는 것이다.


</p><p data-translation="<span>Reportero de Haha Tae thheo@hankookilbo.com</span>">허택회 기자 

                                            thheo@hankookilbo.com


</p><p data-translation="<span>Reportero de Kim Jung Woo wookim@hankookilbo.com</span>">김정우 기자 wookim@hankookilbo.com




                                        </p>

                                    </article><article>
                                        
                                        <p data-translation="<span>Copyright © Korea Times Newspaper Todos los derechos reservados y la redistribución prohibida</span>">
                                            저작권자 © 한국일보 무단전재 및 재배포 금지
                                        </p>
                                    </article></div>