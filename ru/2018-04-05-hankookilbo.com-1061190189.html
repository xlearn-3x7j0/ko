<div id="readability-page-1" class="page"><h3 class="title" data-translation="<span><span id=&quot;_0&quot;>Если</span>оружие ИИ совершило преступление против человека, кто несет ответственность?</span>"><span id="_0" data-translation="Если">AI</span> 무기가 반인류 범죄를 저지른다면 책임자는 누구?</h3><div>
                                        <p data-translation="<span>
                                            </span>">
                                            </p><h4 data-translation="<span><span id=&quot;_1&quot;>Если</span>оружие ИИ совершило преступление против человека, кто несет ответственность?</span>"><span id="_1" data-translation="Если">AI</span> 무기가 반인류 범죄를 저지른다면 책임자는 누구?</h4>

                                        <p data-translation="<span></span>"></p>


                                    </div><article>
                                        
                                        
                                        <p data-translation="<span>Регистрация: 2018.04.05 18:14 Редакция: 2018.04.05 18:15</span>">등록 : 2018.04.05 18:14
                                            
                                                수정 : 2018.04.05 18:15
                                                
                                            </p>
                                    </article><article id="article-body">

                                        

                                        <article>
                                            <div>
                                                <center>

                                        <img src="http://image.hankookilbo.com/i.aspx?Guid=966b2343c2374a788357df4a620c099b&amp;Month=201803&amp;size=640" alt="">

                                         </center>
                                            </div>

                                            <p data-translation="<span>Getty Images Bank</span>">게티이미지뱅크</p>

                                        </article>
                                        
                                              
                                        
                                        <p data-translation="<span>Эксперты Всемирного Робота КАИСТ (<span id=&quot;_2&quot;>KAIST</span>) Защита искусственный интеллект (<span id=&quot;_3&quot;>ИИ</span>) слитые исследовательские центры<span id=&quot;_4&quot;>AI</span>и выразил обеспокоенность тем , что развитие использования «Смертельные роботы<span id=&quot;_5&quot;>ИИ</span>было перемежаются спорами вокруг этики.</span>">세계 로봇 전문가들이 카이스트(<span id="_2" data-translation="KAIST">KAIST</span>) 국방인공지능(<span id="_3" data-translation="ИИ">AI</span>) 융합연구센터가 <span id="_4" data-translation="AI">AI</span>를 활용한 ‘살상 로봇’을 개발할 수 있다고 우려를 표하면서 <span id="_5" data-translation="ИИ">AI</span> 윤리를 둘러싼 논란이 불거지고 있다.</p><p data-translation="<span>Помимо «оружия самоуничтожения не будет исследование противоречит человеческому достоинству» является выяснением КАИСТА, во всем мире<span id=&quot;_6&quot;>AI</span>этики , установленная еще в малышах ситуации<span id=&quot;_7&quot;>AI</span>опасается , что неприятности непредвиденного неисправность и т.д. , и злоупотребление могут bulgeojil напомнить.</span>">“자율살상무기 등 인간의 존엄성에 반하는 연구를 하지 않을 것”이란 카이스트 해명과는 별개로, 전 세계적으로 <span id="_6" data-translation="AI">AI</span> 윤리 제정이 아직 걸음마인 상황에서 <span id="_7" data-translation="AI">AI</span> 오작동ㆍ남용 등으로 예기치 못한 문제가 불거질 수 있다는 우려에서다.</p><p data-translation="<span>Радикально продвижения к<span id=&quot;_8&quot;>ИИ</span>, в отличие от технологии,<span id=&quot;_9&quot;>AI</span>этика все еще находится в зачаточном уровне. « Этика<span id=&quot;_10&quot;>AI</span>разделена на этику разработчиков, этику пользователей и этику, которой должен следовать<span id=&quot;_11&quot;>AI</span>, за исключением этики разработчиков», - сказал представитель Hansang Tech Frontier .</span>">비약적으로 발전하는 <span id="_8" data-translation="ИИ">AI</span> 기술과 달리, <span id="_9" data-translation="AI">AI</span> 윤리 문제는 아직 초보적인 수준에 머물러 있다. 한상기 테크프론티어 대표는 "<span id="_10" data-translation="AI">AI</span> 윤리는 개발자 윤리, 사용자 윤리, <span id="_11" data-translation="AI">AI</span>가 지켜야 할 윤리 등으로 구분되는데, 개발자 윤리를 제외하곤 아직 별다른 진전이 없다"고 말했다. 

</p><p data-translation="<span>Эксперты собрались в Риме в мае прошлого года 1-ацил California США<span id=&quot;_12&quot;>AI</span>«Asilomar , содержащие 23 направлений, в том числе направления D<span id=&quot;_13&quot;>AI</span>naenwat принцип», но все<span id=&quot;_14&quot;>AI</span>о разработчиках. В этом принципе мы должны развивать интеллектуальный, а не интеллектуальный интеллект. Мы должны проектировать<span id=&quot;_15&quot;>ИИ</span>для достижения человеческих ценностей. Мы должны избегать конкуренции с оружием в автоматическом режиме с<span id=&quot;_16&quot;>ИИ</span>. Более 2300 специалистов, в том числе генеральный директор Ilren Musk Tesla , д-р Стивен Хокинг недавно, и<span id=&quot;_17&quot;>генеральный директор</span>Demise Hausbys Google Deep Mind, Alpha<span id=&quot;_18&quot;>Corporation</span>, подписали его.</span>">지난해 1월 미국 캘리포니아 아실로마에서 전문가들이 모여 <span id="_12" data-translation="AI">AI</span> 연구개발 방향 등 23개 방향을 담은 '아실로마 <span id="_13" data-translation="AI">AI</span> 원칙'을 내놨지만, 모두 <span id="_14" data-translation="AI">AI</span> 개발자에 대한 내용이다. 이 원칙에도 ▦방향성이 없는 지능이 아니라 유익한 지능을 개발한다 ▦인류의 가치에 부합하는 쪽으로 <span id="_15" data-translation="ИИ">AI</span>를 설계해야 한다 ▦<span id="_16" data-translation="ИИ">AI</span>를 이용한 자동화 무기 경쟁은 피해야 한다 같은 선언적 내용이 담겼을 뿐이다. 일론 머스크 테슬라 최고경영자(<span id="_17" data-translation="генеральный директор">CEO</span>), 최근 작고한 스티븐 호킹 박사, 알파고를 개발한 데미스 허사비스 구글 딥마인드 <span id="_18" data-translation="Corporation">CEO</span> 등 2,300여명의 전문가가 여기에 서명했다. 

</p><p data-translation="<span>Jangwooseok Hyundai институт экономических исследований исследователь, «дроны (Л) в бомбардировке, нажав на кнопку последнего запуска ракеты<span id=&quot;_19&quot;>ИИ</span>отпуска оставит большую дискуссию в рамках международного сообщества» и «Неподвижный<span id=&quot;_20&quot;>ИИ</span>этика , которые должен соблюдаться и универсальные нормы , что Интернационал Вполне возможно, что если<span id=&quot;_21&quot;>ИИ</span>предоставит автономные функции принятия решений, даже если соглашение не будет достигнуто, будут ситуации, в которых невозможно контролировать ».</span>">장우석 현대경제연구원 연구위원은 “드론(무인기) 폭격에서 마지막 미사일 발사 버튼을 누르는 것까지 <span id="_19" data-translation="ИИ">AI</span>에게 맡길 것인가를 두고 국제사회의 논란이 크다”며 “아직 <span id="_20" data-translation="ИИ">AI</span>가 지켜야 할 윤리ㆍ보편적 규범이 무엇인지 국제합의조차 이뤄지지 않은 상황에서 <span id="_21" data-translation="ИИ">AI</span>에게 자율의사 결정 기능까지 부여하면 통제할 수 없는 상황이 발생할 수도 있다”고 우려했다. 

</p><p data-translation="<span>По этой причине Совет ООН по правам человека рекомендует проводить саморазрушающее испытание оружия, производство и передачу технологии до тех пор, пока не будут установлены международные нормы. Ким Юн-джунг, исследователь из Корейского института науки и техники, оценки и планирования, сказал: «Чем больше технологий<span id=&quot;_22&quot;>ИИ</span>развивается, тем больше автономное принятие решений<span id=&quot;_23&quot;>ИИ</span>может нанести ущерб человечеству. Если вы дадите<span id=&quot;_24&quot;>AI</span>право нарушать, это вызовет в обществе большую путаницу ».</span>">이런 이유로 유엔 인권이사회는 관련한 국제규범이 형성되기 전까지 자율살상무기 실험ㆍ생산ㆍ기술이전 자제를 권고하고 있다. 김윤정 한국과학기술기획평가원 연구위원도 "<span id="_22" data-translation="ИИ">AI</span> 기술이 발달할수록 <span id="_23" data-translation="ИИ">AI</span>가 자율적으로 내린 의사결정이 인류에게 해를 끼치는 경우가 증대될 수 있다"며 "자율살상무기시스템이나 경찰 로봇 등 인류의 기본권을 직접 침해할 수 있는 권한까지 <span id="_24" data-translation="AI">AI</span>에게 부여한다면 사회에 큰 혼란을 가져올 것"이라고 말했다. 

</p><p data-translation="<span><span id=&quot;_25&quot;></span>Кто будет наказан, когда ИИ совершает преступления против человечности, по-прежнему остается проблемой. Наука и недавние «правовые и этические проблемы искусственного интеллекта эпохи» Политика Института Технологии сообщает &quot;<span id=&quot;_26&quot;>AI</span>, когда робот iphyeoteul вред другому лицу в соответствии с инструкцией владельца jilji хозяина обязанности робота, чтобы следовать за неправильную команду<span id=&quot;_27&quot;>AI</span>производители сделал И нужно ли это делать ».</span>"><span id="_25" data-translation="">AI</span>가 인륜에 반하는 범죄를 저질렀을 때 누구를 처벌할 것인지도 아직 미해결 과제다. 과학기술정책연구원은 최근 '인공지능 시대의 법적ㆍ윤리적 쟁점' 보고서에서 "<span id="_26" data-translation="AI">AI</span> 로봇이 소유주의 명령에 따라 타인에게 상해를 입혔을 때 그 책임을 로봇 주인이 질지, 잘못된 명령을 따르도록 <span id="_27" data-translation="AI">AI</span>를 만든 제작자가 져야 하는지 등이 향후 쟁점이 될 것"이라고 내다봤다. 

</p><p data-translation="<span><span id=&quot;_28&quot;>Поскольку</span>проблема этики ЭКС стала неотложной задачей, всемирно известные университеты создают курсы этики<span id=&quot;_29&quot;>AI</span>для развития специалистов . Гарвардский университет и Массачусетский технологический институт (<span id=&quot;_30&quot;>Массачусетский</span>технологический институт ) начнут совместную лекцию по регулированию и этике<span id=&quot;_31&quot;>ИИ в</span>этом году . Стэнфордский университет готовится открыть курс «Этика компьютерной науки», цель которого - начать в следующем году. Ким сказала: « Промышленные правила, которые препятствуют развитию<span id=&quot;_32&quot;>ИИ,</span>должны быть решены, но этические правила следует учитывать».</span>"><span id="_28" data-translation="Поскольку">AI</span> 윤리 제정 문제가 시급한 과제로 떠오른 만큼, 해외 유명 대학에선 전문가 양성을 위해 속속 <span id="_29" data-translation="AI">AI</span> 윤리 과목을 개설하고 있다. 미국 하버드대와 매사추세츠공대(<span id="_30" data-translation="Массачусетский">MIT</span>)는 올해부터 <span id="_31" data-translation="ИИ в">AI</span> 규제와 윤리를 주제로 공동 강의를 시작한다. 미국 스탠퍼드대는 내년 개강을 목표로 '컴퓨터 사이언스 윤리학' 과목 개설을 준비 중이다. 김 연구위원은 "<span id="_32" data-translation="ИИ,">AI</span> 발전을 가로막는 산업규제는 풀어야 하지만, 윤리 규제는 이와 별로도 생각하고 계속 고민해야 한다"고 말했다. 

</p><p data-translation="<span>Автор: Tae-Sup Tae libertas@hankookilbo.com</span>">변태섭기자 libertas@hankookilbo.com


                                        </p>

                                    </article><article>
                                        
                                        <p data-translation="<span>Copyright © Korea Times Newspaper Все права защищены и перераспределение запрещено</span>">
                                            저작권자 © 한국일보 무단전재 및 재배포 금지
                                        </p>
                                    </article></div>