<div id="readability-page-1" class="page"><h3 class="title" data-translation="<span>The 'killer robot' debate that has become reality</span>">현실이 돼 버린 ‘킬러 로봇’ 논쟁</h3><article>
                                        
                                        
                                        <p data-translation="<span>Registration: 2018.04.05 19:13 Revision: 2018.04.05 19:21</span>">등록 : 2018.04.05 19:13
                                            
                                                수정 : 2018.04.05 19:21
                                                
                                            </p>
                                    </article><article id="article-body">

                                        

                                        <article>
                                            <div>
                                                <center>

                                        <img src="http://image.hankookilbo.com/i.aspx?Guid=3c74dd92cbce4befaf94d6fea41a1f01&amp;Month=201804&amp;size=640" alt="">

                                         </center>
                                            </div>

                                            <p data-translation="<span>The propaganda of the beginning of the Terminator future war</span>">터미네이터 미래 전쟁의 시작의 선전물</p>

                                        </article>
                                        
                                              
                                        
                                        <p data-translation="<span>&quot;A machine can kill a man by his own judgment.&quot;</span>">“기계가 스스로의 판단 만으로 사람을 죽일 수 있나.”
</p><p data-translation="<span>The question of the Hollywood movie &quot;Terminator&quot; series, which was treated only as a fancy movie 34 years ago, came into existence in 2018 as a matter of existence for mankind.</span>">34년전 공상영화로만 취급 받았던 헐리우드 영화 ‘터미네이터’ 시리즈의 질문이 2018년 인류에게 실존 문제로 다가왔다.</p><p data-translation="<span>The human desire for a stronger weapon began to rise with the artificial intelligence (<span id=&quot;_0&quot;>AI</span>), which led to the development of weapons to be judged and driven by the scientists. In addition, the center of the controversy is not only the cutting-edge company of Silicon Valley in the United States, but also the Korean Institute of Science and Technology (KAIST), a symbol of Korean science and technology.</span>">더 강한 무기에 대한 인간 욕망이 인공지능(<span id="_0" data-translation="AI">AI</span>)을 장착해 스스로 판단하고 구동하는 무기 개발로 쏠리면서, 과학자 집단에서 윤리 논쟁이 달아오르기 시작했다. 게다가 논쟁의 중심에는 미국 실리콘밸리의 최첨단 기업 구글은 물론이고 한국 과학기술의 상징인 한국과학기술원(카이스트)까지 휘말려 들고 있다.</p><p data-translation="<span>The New York Times (<span id=&quot;_1&quot;>NYT</span>) reported on Tuesday (April 4 ) a petition signed by 3,100 Googler employees, including dozens of top-level engineers , to be sent to Sun Darphy Chai's chief executive<span id=&quot;_2&quot;>officer</span>. The content was simple. &quot;We believe that Google should not engage in war.&quot; The collective action of Google employees is due to the company's Pentagon and the ongoing 'Maven' project. Maven is a Pentagon pilot program that aims to improve the hitting capabilities of US Air Force unmanned fighter (drones) with Google's cloud-based<span id=&quot;_3&quot;>AI</span>technology.</span>">뉴욕타임스(<span id="_1" data-translation="NYT">NYT</span>)는 4일(현지시간) 최고위급 엔지니어 수 십명을 포함, 구글직원 3,100명이 서명해 순다르 피차이 최고경영자(<span id="_2" data-translation="officer">CEO</span>)에게 보낼 청원서를 보도했다. 내용은 간단했다. “우리는 구글이 전쟁 사업에 참여해선 안 된다고 믿는다” 였다. 구글 직원의 집단 행동은 이 회사가 미 국방부(펜타곤)와 진행 중인 ‘메이븐’ 프로젝트 때문이다. 메이븐은 구글의 클라우드 기반 인공지능(<span id="_3" data-translation="AI">AI</span>) 기술로 미 공군 무인전투기(드론)의 타격 능력 향상을 꾀하려는 펜타곤의 파일럿 프로그램이다. 

</p><p data-translation="<span><span id=&quot;_4&quot;></span>According to the NYT , the message from Google engineers is clear. No matter how you target the enemy,<span id=&quot;_5&quot;>AI</span>technology development aimed at destroying lives is aligned with the goal of the Google organization, &quot;<span id=&quot;_6&quot;>Do not</span> <span id=&quot;_7&quot;>be</span> <span id=&quot;_8&quot;>Evil</span>.&quot; &quot;It is unacceptable to build a military surveillance system that can result in catastrophic consequences for cooperating with the government,&quot; he said.<span id=&quot;_9&quot;>The NYT</span>also analyzed that &quot;cultural clashes between Silicon Valley and government that have advanced technology in the increasing use of<span id=&quot;_10&quot;>AI</span>technology for military purposes.&quot;</span>"><span id="_4" data-translation="">NYT</span>에 따르면 구글 엔지니어들의 메시지는 뚜렷하다. 아무리 적군을 대상으로 한다 해도 인명살상이 목적의 <span id="_5" data-translation="AI">AI</span> 기술 개발은 ‘사악해지지 말자(<span id="_6" data-translation="Do not">Don’t</span> <span id="_7" data-translation="be">be</span> <span id="_8" data-translation="Evil">Evil</span>)’인 구글 조직의 목표와 정면 배치된다는 얘기다. 이들은 “정부에 협력한다는 명분으로 군사적 감시, 나아가 치명적인 결과를 낳을 수도 있는 기술을 구축하는 것은 받아들일 수 없다”고 밝혔다. <span id="_9" data-translation="The NYT">NYT</span>도 “<span id="_10" data-translation="AI">AI</span> 기술의 군사 목적 활용이 점점 늘어나는 상황에서 첨단 기술을 보유한 실리콘밸리와 정부 간에 벌어지는 문화적 충돌”이라고 분석했다.

</p><p data-translation="<span>Unfortunately, on the same day, Korea was caught up in the &quot;terminator&quot; debate. Fifty foreign prominent robotologists sent a warning letter to KAIST. The beginning was due to the 'Defense Artificial Intelligence Convergence Research Center' jointly opened by KAIST and Hanhwa Systems in late February. In a letter to the British Financial Times (<span id=&quot;_11&quot;>FT</span>), robotologists have spoken directly to &quot;killer robot concerns,&quot; saying, &quot;Until KAIST's commitment to develop weapons that autonomously determine without human control, I will completely reject all collaborative research. &quot;</span>">공교롭게 같은 날 한국도 ‘터미네이터’ 논쟁에 휘말렸다. 외국의 저명한 로봇학자 50명이 카이스트에 경고 서한을 보낸 것. 발단은 카이스트와 한화시스템이 지난 2월 말 공동 개소한 ‘국방인공지능융합연구센터’ 때문이었다. 영국 파이낸셜타임스(<span id="_11" data-translation="FT">FT</span>)가 공개한 서한에서 로봇학자들은 ‘킬러로봇 우려’까지 직접 거론하며, “인간의 유의미한 통제 없이 자율적으로 결정하는 무기를 개발하지 않겠다는 확약을 카이스트 총장이 할 때까지 우리는 모든 공동연구를 전면 거부할 것”이라고 선언했다. 

</p><p data-translation="<span>KAIST has no intention of developing a killer robot. A school official said, &quot;We sent an official letter saying,&quot; We did not conduct research on weapons of mass destruction or killer robots for offensive purposes. &quot; &quot;We will once again emphasize that we will not conduct research activities that are contrary to human dignity, such as the development of autonomous weapons lacking control.&quot;</span>">카이스트는 킬러 로봇 개발 의사가 전혀 없다는 입장이다. 학교 관계자는 “지난달 19일 ‘공격용 대량살상 무기나 킬러 로봇 개발 목적의 연구 수행이 아니다’라는 공문을 보냈다”고 해명했다. 또 “통제력이 결여된 자율무기 개발 등 인간 존엄성에 어긋나는 연구활동을 수행하지 않을 것임을 다시 한번 강조한다”고 밝혔다.

</p><p data-translation="<span>Experts analyze the controversy in Google and KAIST as the beginning of the &quot;terminator&quot; debate that will last for some time to come. At the UN Convention on Conventional Weapons Convention (<span id=&quot;_12&quot;>CCW</span>) in November last year , UN experts and NGOs discussed ways to ban<span id=&quot;_13&quot;>AI-</span>equipped weapons. However, the engineers responsible for actual development raised official ethical issues This is because this is the first time.</span>">전문가들은 구글과 카이스트에서 벌어진 논쟁이 향후 상당기간 이어질 ‘터미네이터’ 논쟁의 시작으로 분석하고 있다. 지난해 11월 유엔 특정재래식무기금지협약(<span id="_12" data-translation="CCW">CCW</span>) 회의에서 <span id="_13" data-translation="AI-">AI</span> 탑재 무기를 금지하는 방안이 유엔 전문가들과 시민단체를 중심으로 논의된 바 있지만, 실제 개발을 담당할 엔지니어들이 공식적으로 윤리 문제를 제기한 것은 이번이 처음이기 때문이다. 

</p><p data-translation="<span>Civilian activists are claiming strong legal restrictions on<span id=&quot;_14&quot;>AI</span>weapons on humanitarian grounds, but experts are showing a gloomy outlook. Humanity greed over the generosity of<span id=&quot;_15&quot;>AI</span>technology, good weapons, and the distrust of the enemy, which is an unstoppable military competition. Australia's prominent philosopher Peter Singer said, &quot;Money-conscious military contractors and geopolitical rivalries around the world can accelerate<span id=&quot;_16&quot;>AI</span>weapons competition.&quot; Former US Assistant Secretary of Defense James Miller also said, &quot;The United States does not have the weapons that<span id=&quot;_17&quot;>AI</span>will automatically judge, but if enemies like Russia or China prepare powerful weapons equipped with<span id=&quot;_18&quot;>AI</span>, the response will be different.&quot;</span>">시민 운동가들은 인도주의적 차원에서 <span id="_14" data-translation="AI">AI</span> 무기에 대한 강력한 법적 규제를 주장하고 있으나, 전문가들은 우울한 전망을 내놓고 있다. <span id="_15" data-translation="AI">AI</span> 기술의 범용성과 우수한 무기에 대한 인간의 탐욕, 그리고 적국에 대한 불신 때문에 멈출 수 없는 군비경쟁이 우려된다는 것이다. 호주의 저명한 철학자 피터 싱어는 “돈에 눈이 먼 군수업자, 각국의 지정학적 경쟁 등이 <span id="_16" data-translation="AI">AI</span> 무기 경쟁을 가속화 시킬 수 있다”고 말했다. 제임스 밀러 전 미국 국방부 차관보도 “미국은 <span id="_17" data-translation="AI">AI</span>가 자동 판단하는 무기를 갖지 않는 게 원칙이지만, 러시아 혹은 중국 같은 적국이 <span id="_18" data-translation="AI">AI</span>를 장착한 강력한 무기를 준비한다면 그 대응은 달라질 것”이라고 우려했다.

</p><p data-translation="<span>The ethical question: &quot;Is it really possible for a machine to judge itself and kill a human being?&quot; It is highly likely that<span id=&quot;_19&quot;>AI</span>technology will be expanded to infinity without proper control before mankind finds the answer .</span>">‘기계가 스스로 판단해 인간을 살해하는 게 과연 온당한가’라는 윤리적 질문에 인류가 해답을 찾기도 전에 <span id="_19" data-translation="AI">AI</span>기술이 적절한 통제 없이 무한대로 확장될 가능성이 높다는 것이다.


</p><p data-translation="<span>Haha Tae reporter thheo@hankookilbo.com</span>">허택회 기자 

                                            thheo@hankookilbo.com


</p><p data-translation="<span>Kim Jung Woo reporter wookim@hankookilbo.com</span>">김정우 기자 wookim@hankookilbo.com




                                        </p>

                                    </article><article>
                                        
                                        <p data-translation="<span>Copyright © Korea Times Newspaper All rights reserved and redistribution prohibited</span>">
                                            저작권자 © 한국일보 무단전재 및 재배포 금지
                                        </p>
                                    </article></div>